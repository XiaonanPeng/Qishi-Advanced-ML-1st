This repository is for the first advanced machine learning study group in Qishi. The following is our syllabus:

### Lecture 0: Introduction
- Understand the overall structure and goals of the course.
- Identify connections between different models.
- Recognize real-world applications of the models discussed.
### Lecture 1: Lasso and Regularization
- Develop a comprehensive understanding of Lasso and its role in regularization.
- Explore how Lasso performs feature selection by enforcing sparsity.
- Understand implicit regularization and its effects on model complexity.
- Examine the relationship between Lasso and boosting techniques.
- Learn efficient computational methods like coordinate descent for solving Lasso problems.
### Lecture 2: Principal Component Analysis(PCA)
- Understand the mathematical foundation of PCA.
- Learn how PCA reduces dimensionality by identifying key components that capture the most variance.
- Explore the connection between PCA and Variational Autoencoders (VAEs) in terms of data representation and dimensionality reduction.
### Lecture 3: Proabilistic Graphcial Model
- Understand the structure and types of probabilistic graphical models (PGMs), including Bayesian and Markov networks.
- Learn how PGMs represent complex distributions and dependencies using graphs.
- Explore the principles of Bayesian machine learning and how PGMs facilitate reasoning under uncertainty.
### Lecture 4: Expectation-Maximization(EM) Algorithm 1
- Understand the fundamentals of the Expectation-Maximization (EM) algorithm.
- Learn the detailed mathematical derivation of the EM algorithm.
- Explore the proofs of inequalities used in EM, such as Jensen's inequality.
- Apply EM to simple applications like estimating parameters with missing data.
- Discuss other common applications of the EM algorithm.
### Lecture 5: EM Algorithm 2 and Gaussian Mixture Model
- Explore advanced applications of the EM algorithm, including the MM (Minorization-Maximization) algorithm.
- Use Gaussian Mixture Models (GMMs) as a key example to understand the practical implementation of EM.
- Delve into the iterative logic and convergence properties of EM in complex scenarios.
### Lecture 6: Variational Inference
- Understand the principles of variational inference.
- Learn how to approximate complex posterior distributions.
### Lecture 7: State Space Model 1: Hidden Markov Model
- Understand the structure of Hidden Markov Models.
### Lecture 8: State Space Model 2: Kalman Filter
- Understand the working of Kalman Filters.
### Lecture 9: State Spcae Model 3: Particle Filter
-
### Lecture 10: MCMC 1: Basic sampling method：Importance Sampling，Gibbs Sampling, MH mehtod
-
### Lecture 11: MCMC 2: Advanced MCMC methods：SMC and HMC
-
### Lecture 12: Variational Auto Encoder(VAE)
-
### Lecture 13: Generative Adversarial Network(GAN)
-
### Lecture 14: Diffusion Model 1: DDPM and DDIM
-
### Lecture 15: Diffusion Model 2: Stable Diffusion
-

### Reference Books:
1. Pattern Recognition and Machine Learning, Bishop
2. The Elements of Statistical Learning, Hastie
3. Computer-Age Statistical Inference, Hastie
4. Probabilistic Machine Learning: Introduction, Murphy 
5. Probabilistic Machine Learning: Advanced Topics, Murphy
6. Probabilistic Deep Learning with Python, Keras and Tensorflow Probability, Sick
7. Time Series Analysis by State Spcae Methods, Durbin
8. MCMC from Scratch, Hanada
9. Statistical Inference, Casella

### Reference Links:
1. 白板推导机器学习：https://www.bilibili.com/video/BV1aE411o7qd/?spm_id_from=333.999.0.0&vd_source=648125ac3e2fb6b31759a16531ce143b
2. 徐亦达机器学习：https://space.bilibili.com/327617676/channel/series
3. 线代启示录： https://ccjou.wordpress.com
4. 科学空间：https://spaces.ac.cn

