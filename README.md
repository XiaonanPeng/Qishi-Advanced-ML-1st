This repository is for the first advanced machine learning study group in Qishi. The following is our syllabus:

### Lecture 0: Introduction
- Understand the overall structure and goals of the course.
- Identify connections between different models.
- Recognize real-world applications of the models discussed.
### Lecture 1: Lasso and Regularization
- Develop a comprehensive understanding of Lasso and its role in regularization.
- Explore how Lasso performs feature selection by enforcing sparsity.
- Understand implicit regularization and its effects on model complexity.
- Examine the relationship between Lasso and boosting techniques.
- Learn efficient computational methods like coordinate descent for solving Lasso problems.
### Lecture 2: Principal Component Analysis(PCA)
- Understand the mathematical foundation of PCA.
- Learn how PCA reduces dimensionality by identifying key components that capture the most variance.
- Explore the connection between PCA and Variational Autoencoders (VAEs) in terms of data representation and dimensionality reduction.
### Lecture 3: Proabilistic Graphcial Model
- Understand the structure and types of probabilistic graphical models.
### Lecture 4: EM Algorithm 1
- Understand the Expectation-Maximization algorithm.
- Learn how to handle incomplete data sets.
- Apply EM to estimate model parameters iteratively.
### Lecture 5: EM Algorithm 2 and Gaussian Mixture Model
- Extend EM to Gaussian Mixture Models.
- Understand clustering through GMMs.
### Lecture 6: Variational Inference
- Understand the principles of variational inference.
- Learn how to approximate complex posterior distributions.
### Lecture 7: State Space Model 1: Hidden Markov Model
- Understand the structure of Hidden Markov Models.
### Lecture 8: State Space Model 2: Kalman Filter
- Understand the working of Kalman Filters.
### Lecture 9: State Spcae Model 3: Particle Filter
-
### Lecture 10: MCMC 1: Basic sampling method：Importance Sampling，Gibbs Sampling, MH mehtod
-
### Lecture 11: MCMC 2: Advanced MCMC methods：SMC and HMC
-
### Lecture 12: Variational Auto Encoder(VAE)
-
### Lecture 13: Generative Adversarial Network(GAN)
-
### Lecture 14: Diffusion Model 1: DDPM and DDIM
-
### Lecture 15: Diffusion Model 2: Stable Diffusion
-

### Reference Books:
1. Pattern Recognition and Machine Learning, Bishop
2. The Elements of Statistical Learning, Hastie
3. Computer-Age Statistical Inference, Hastie
4. Probabilistic Machine Learning: Introduction, Murphy 
5. Probabilistic Machine Learning: Advanced Topics, Murphy
6. Probabilistic Deep Learning with Python, Keras and Tensorflow Probability, Sick
7. Time Series Analysis by State Spcae Methods, Durbin
8. MCMC from Scratch, Hanada
9. Statistical Inference, Casella

### Reference Links:
1. 白板推导机器学习：https://www.bilibili.com/video/BV1aE411o7qd/?spm_id_from=333.999.0.0&vd_source=648125ac3e2fb6b31759a16531ce143b
2. 徐亦达机器学习：https://space.bilibili.com/327617676/channel/series
3. 线代启示录： https://ccjou.wordpress.com
4. 科学空间：https://spaces.ac.cn

